{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Feedback Analysis Project\n",
    "\n",
    "## Business Impact: Product Improvement through Customer Sentiment Analysis\n",
    "\n",
    "In this project, we'll analyze Amazon product reviews to help businesses understand:\n",
    "1. Overall customer satisfaction\n",
    "2. Key product features that customers love/hate\n",
    "3. Common pain points and improvement areas\n",
    "4. Sentiment trends over time\n",
    "\n",
    "This analysis can directly impact business decisions by:\n",
    "- Identifying priority areas for product improvement\n",
    "- Understanding customer preferences\n",
    "- Tracking the impact of product changes\n",
    "- Improving customer satisfaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/aditya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/aditya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/aditya/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from collections import Counter\n",
    "import re\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import gzip\n",
    "import json\n",
    "import requests\n",
    "from io import BytesIO\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_palette('husl')\n",
    "\n",
    "import nltk\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Initialize sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection and Preparation\n",
    "\n",
    "We'll use the Amazon Product Reviews dataset, which contains millions of reviews across different product categories. For this project, we'll focus on a specific product category to keep the analysis manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Amazon reviews...\n",
      "\n",
      "Dataset Information:\n",
      "Number of reviews: 1000\n",
      "\n",
      "Sample reviews:\n",
      "                                         review_text  rating\n",
      "0  We got this GPS for my husband who is an (OTR)...     5.0\n",
      "1  I'm a professional OTR truck driver, and I bou...     1.0\n",
      "2  Well, what can I say.  I've had this unit in m...     3.0\n",
      "3  Not going to write a long review, even thought...     2.0\n",
      "4  I've had mine for a year and here's what we go...     1.0\n",
      "\n",
      "Rating Distribution:\n",
      "rating\n",
      "1.0     88\n",
      "2.0     48\n",
      "3.0     94\n",
      "4.0    188\n",
      "5.0    582\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def load_amazon_reviews(category='Electronics', max_reviews=1000):\n",
    "    \"\"\"Load Amazon reviews for a specific category\"\"\"\n",
    "    # URL for the Amazon review dataset\n",
    "    url = f\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_{category}_5.json.gz\"\n",
    "    \n",
    "    try:\n",
    "        # Download the file\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "        \n",
    "        # Read the gzipped file\n",
    "        with gzip.GzipFile(fileobj=BytesIO(response.content)) as f:\n",
    "            # Read and parse the JSON lines\n",
    "            reviews = []\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= max_reviews:\n",
    "                    break\n",
    "                review = json.loads(line)\n",
    "                reviews.append({\n",
    "                    'review_id': review.get('reviewerID', ''),\n",
    "                    'product': review.get('asin', ''),\n",
    "                    'review_text': review.get('reviewText', ''),\n",
    "                    'rating': review.get('overall', 0),\n",
    "                    'date': review.get('reviewTime', ''),\n",
    "                    'category': category\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(reviews)\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading data: {e}\")\n",
    "        print(\"\\nUsing sample data instead...\")\n",
    "        \n",
    "        # Fallback to sample data if download fails\n",
    "        return pd.DataFrame([\n",
    "            {\n",
    "                'review_id': 'A1B2C3',\n",
    "                'product': 'Sample Product',\n",
    "                'review_text': 'This is a great product with amazing features. The battery life is excellent!',\n",
    "                'rating': 5,\n",
    "                'date': '2024-01-15',\n",
    "                'category': 'Electronics'\n",
    "            },\n",
    "            {\n",
    "                'review_id': 'D4E5F6',\n",
    "                'product': 'Sample Product',\n",
    "                'review_text': 'Good product but the camera quality could be better.',\n",
    "                'rating': 3,\n",
    "                'date': '2024-01-20',\n",
    "                'category': 'Electronics'\n",
    "            }\n",
    "        ])\n",
    "\n",
    "# Load the data\n",
    "print(\"Loading Amazon reviews...\")\n",
    "df = load_amazon_reviews(category='Electronics', max_reviews=1000)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"\\nDataset Information:\")\n",
    "print(f\"Number of reviews: {len(df)}\")\n",
    "print(f\"\\nSample reviews:\")\n",
    "print(df[['review_text', 'rating']].head())\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nRating Distribution:\")\n",
    "print(df['rating'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Preprocessing\n",
    "\n",
    "Let's clean and preprocess the review data to prepare it for analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned Dataset Information:\n",
      "Number of reviews after cleaning: 1000\n",
      "\n",
      "Sample of cleaned reviews:\n",
      "                                        cleaned_text  rating\n",
      "0  we got this gps for my husband who is an otr o...     5.0\n",
      "1  i m a professional otr truck driver and i boug...     1.0\n",
      "2  well what can i say i ve had this unit in my t...     3.0\n",
      "3  not going to write a long review even thought ...     2.0\n",
      "4  i ve had mine for a year and here s what we go...     1.0\n"
     ]
    }
   ],
   "source": [
    "def clean_review_text(text):\n",
    "    \"\"\"Clean and preprocess review text\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Clean the review text\n",
    "df['cleaned_text'] = df['review_text'].apply(clean_review_text)\n",
    "\n",
    "# Convert date strings to datetime objects\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "# Remove rows with missing values\n",
    "df = df.dropna(subset=['review_text', 'rating', 'date'])\n",
    "\n",
    "print(\"\\nCleaned Dataset Information:\")\n",
    "print(f\"Number of reviews after cleaning: {len(df)}\")\n",
    "print(\"\\nSample of cleaned reviews:\")\n",
    "print(df[['cleaned_text', 'rating']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing Exercise\n",
    "\n",
    "Let's create a more comprehensive preprocessing function that includes:\n",
    "1. Removing stopwords\n",
    "2. Lemmatization\n",
    "3. Handling contractions\n",
    "4. Removing URLs and email addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed text: None\n"
     ]
    }
   ],
   "source": [
    "def advanced_preprocess(text):\n",
    "    #TODO: you can use this function to preprocess the text\n",
    "\n",
    "    #YOUR CODE HERE\n",
    "    pass\n",
    "# Test your implementation\n",
    "sample_text = \"The product's battery life is amazing! Check it out at www.example.com or email us at test@example.com\"\n",
    "processed_text = advanced_preprocess(sample_text)\n",
    "print(\"Processed text:\", processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sentiment Analysis Exercise\n",
    "\n",
    "Create a function that:\n",
    "1. Analyzes sentiment of reviews\n",
    "2. Categorizes reviews as Positive, Negative, or Neutral\n",
    "3. Calculates confidence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the sentiment analysis function\n",
    "def analyze_sentiment_with_confidence(text):\n",
    "    \"\"\"Analyze sentiment with confidence scores\"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "test_reviews = [\n",
    "    \"This product is absolutely amazing! Best purchase ever.\",\n",
    "    \"The quality is okay, nothing special.\",\n",
    "    \"Terrible product, would not recommend.\"\n",
    "]\n",
    "\n",
    "for review in test_reviews:\n",
    "    result = analyze_sentiment_with_confidence(review)\n",
    "    print(f\"\\nReview: {review}\")\n",
    "    print(f\"Analysis: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Extraction Exercise\n",
    "\n",
    "Create a function that:\n",
    "1. Identifies product features mentioned in reviews\n",
    "2. Extracts sentiment for each feature\n",
    "3. Calculates feature importance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Implement the feature extraction function\n",
    "def extract_features_with_sentiment(text):\n",
    "    #YOUR CODE HERE\n",
    "    pass\n",
    "    \n",
    "# Test your implementation\n",
    "test_review = \"The battery life is excellent, but the camera quality needs improvement. The screen is beautiful though.\"\n",
    "features = extract_features_with_sentiment(test_review)\n",
    "print(\"Extracted features with sentiment:\")\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization Exercise\n",
    "\n",
    "Create functions to visualize:\n",
    "1. Sentiment distribution\n",
    "2. Feature importance\n",
    "3. Sentiment trends over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement visualization functions\n",
    "def plot_sentiment_distribution(sentiments):\n",
    "    \"\"\"Plot sentiment distribution\"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "def plot_feature_importance(features):\n",
    "    \"\"\"Plot feature importance\"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "def plot_sentiment_trends(dates, sentiments):\n",
    "    \"\"\"Plot sentiment trends over time\"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# Test your implementations\n",
    "print(\"Implement and test your visualization functions here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Business Insights Exercise\n",
    "\n",
    "Create a function that generates business insights by:\n",
    "1. Identifying top positive and negative features\n",
    "2. Calculating improvement priorities\n",
    "3. Generating actionable recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the business insights function\n",
    "def generate_business_insights(reviews_data):\n",
    "    \"\"\"Generate business insights from review data\"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "sample_data = {\n",
    "    'reviews': [\n",
    "        {'text': 'Great battery life but poor camera', 'rating': 4},\n",
    "        {'text': 'Amazing screen quality', 'rating': 5},\n",
    "        {'text': 'Battery drains too fast', 'rating': 2}\n",
    "    ]\n",
    "}\n",
    "\n",
    "insights = generate_business_insights(sample_data)\n",
    "print(\"Generated insights:\")\n",
    "print(insights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Main Project Exercise\n",
    "\n",
    "Now it's your turn! Create a complete customer feedback analysis system that:\n",
    "\n",
    "1. Loads and processes real Amazon review data\n",
    "2. Implements all the functions from previous exercises\n",
    "3. Generates comprehensive business insights\n",
    "4. Creates visualizations for key findings\n",
    "5. Produces a detailed analysis report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the complete analysis system\n",
    "\n",
    "def analyze_customer_feedback(category='Electronics', max_reviews=1000):\n",
    "    \"\"\"Complete customer feedback analysis system\"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# Run the analysis\n",
    "results = analyze_customer_feedback()\n",
    "print(\"Analysis complete! Check the results above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Generate detailed business recommendations using a local LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def generate_detailed_recommendations(insights):\n",
    "    \"\"\"\n",
    "    Generate detailed business recommendations using a local LLM\n",
    "    \"\"\"\n",
    "    # Load a smaller model that can run locally (e.g., GPT-2)\n",
    "    model_name = \"gpt2\"  # You can also try \"facebook/opt-125m\" for a smaller model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    # Create a text generation pipeline\n",
    "    generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "    \n",
    "    # Prepare the prompt\n",
    "    prompt = f\"\"\"Based on the following customer feedback analysis, provide detailed business recommendations:\n",
    "                {insights}\n",
    "                Please provide specific, actionable recommendations for each area of improvement:\"\"\"\n",
    "\n",
    "    # Generate recommendations\n",
    "    try:\n",
    "        # Generate text with the model\n",
    "        generated_text = generator(\n",
    "            prompt,\n",
    "            max_length=500,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True\n",
    "        )[0]['generated_text']\n",
    "        \n",
    "        # Extract just the recommendations part\n",
    "        recommendations = generated_text.split(\"Please provide specific, actionable recommendations for each area of improvement:\")[1].strip()\n",
    "        \n",
    "        return recommendations\n",
    "    except Exception as e:\n",
    "        return f\"Error generating recommendations: {str(e)}\"\n",
    "\n",
    "# Test the function with the insights from our analysis\n",
    "print(\"Generating detailed business recommendations...\")\n",
    "detailed_recommendations = generate_detailed_recommendations(results['insights'])\n",
    "print(\"\\nDetailed Business Recommendations:\")\n",
    "print(detailed_recommendations)\n",
    "\n",
    "# Optional: Save recommendations to a file\n",
    "with open('business_recommendations.txt', 'w') as f:\n",
    "    f.write(detailed_recommendations)\n",
    "print(\"\\nRecommendations have been saved to 'business_recommendations.txt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions\n",
    "\n",
    "Here are the solutions to all exercises:\n",
    "\n",
    "### Exercise 2: Advanced Preprocessing\n",
    "```python\n",
    "def advanced_preprocess(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Handle contractions\n",
    "    contractions = {\n",
    "        \"'s\": \" is\",\n",
    "        \"'re\": \" are\",\n",
    "        \"'t\": \" not\",\n",
    "        \"'d\": \" would\",\n",
    "        \"'ll\": \" will\",\n",
    "        \"'ve\": \" have\"\n",
    "    }\n",
    "    for contraction, expansion in contractions.items():\n",
    "        text = text.replace(contraction, expansion)\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "```\n",
    "\n",
    "### Exercise 3: Sentiment Analysis\n",
    "```python\n",
    "def analyze_sentiment_with_confidence(text):\n",
    "    scores = sia.polarity_scores(text)\n",
    "    \n",
    "    # Determine sentiment category\n",
    "    if scores['compound'] >= 0.05:\n",
    "        sentiment = 'Positive'\n",
    "    elif scores['compound'] <= -0.05:\n",
    "        sentiment = 'Negative'\n",
    "    else:\n",
    "        sentiment = 'Neutral'\n",
    "    \n",
    "    # Calculate confidence\n",
    "    confidence = abs(scores['compound'])\n",
    "    \n",
    "    return {\n",
    "        'sentiment': sentiment,\n",
    "        'confidence': confidence,\n",
    "        'scores': scores\n",
    "    }\n",
    "```\n",
    "\n",
    "### Exercise 4: Feature Extraction\n",
    "```python\n",
    "# Define feature keywords\n",
    "    features = {\n",
    "        'battery': ['battery', 'life', 'charge'],\n",
    "        'camera': ['camera', 'photo', 'picture'],\n",
    "        'screen': ['screen', 'display', 'resolution'],\n",
    "        'performance': ['performance', 'speed', 'fast', 'slow'],\n",
    "        'software': ['software', 'app', 'interface']\n",
    "    }\n",
    "\n",
    "    # Preprocess and tokenize text into sentences\n",
    "    text = advanced_preprocess(text)\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Extract features and their sentiment\n",
    "    feature_sentiments = {}\n",
    "    for feature, keywords in features.items():\n",
    "        # Find sentences containing any of the feature's keywords\n",
    "        relevant_sentences = [\n",
    "            sentence for sentence in sentences\n",
    "            if any(keyword in sentence for keyword in keywords)\n",
    "        ]\n",
    "        if relevant_sentences:\n",
    "            # Combine relevant sentences and analyze sentiment\n",
    "            feature_text = ' '.join(relevant_sentences)\n",
    "            sentiment = analyze_sentiment_with_confidence(feature_text)\n",
    "            feature_sentiments[feature] = sentiment\n",
    "\n",
    "    return feature_sentiments\n",
    "# Test your implementation\n",
    "test_reviews = [\n",
    "    \"This product is absolutely amazing! Best purchase ever.\",\n",
    "    \"The quality is okay, nothing special.\",\n",
    "    \"Terrible product, would not recommend.\"\n",
    "]\n",
    "\n",
    "for review in test_reviews:\n",
    "    result = analyze_sentiment_with_confidence(review)\n",
    "    print(f\"\\nReview: {review}\")\n",
    "    print(f\"Analysis: {result}\")\n",
    "```\n",
    "\n",
    "### Exercise 5: Visualizations\n",
    "```python\n",
    "def plot_sentiment_distribution(sentiments):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(sentiments, bins=20)\n",
    "    plt.title('Distribution of Review Sentiments')\n",
    "    plt.xlabel('Sentiment Score')\n",
    "    plt.ylabel('Number of Reviews')\n",
    "    plt.show()\n",
    "\n",
    "def plot_feature_importance(features):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x=list(features.keys()), y=[f['confidence'] for f in features.values()])\n",
    "    plt.title('Feature Importance')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "def plot_sentiment_trends(dates, sentiments):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(dates, sentiments)\n",
    "    plt.title('Sentiment Trends Over Time')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Average Sentiment')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "### Exercise 6: Business Insights\n",
    "```python\n",
    "def generate_business_insights(reviews_data):\n",
    "    insights = []\n",
    "    \n",
    "    # Analyze overall sentiment\n",
    "    sentiments = [analyze_sentiment_with_confidence(review['text']) for review in reviews_data['reviews']]\n",
    "    avg_sentiment = np.mean([s['scores']['compound'] for s in sentiments])\n",
    "    insights.append(f\"Overall customer satisfaction: {avg_sentiment:.2f}\")\n",
    "    \n",
    "    # Analyze features\n",
    "    feature_sentiments = {}\n",
    "    for review in reviews_data['reviews']:\n",
    "        features = extract_features_with_sentiment(review['text'])\n",
    "        for feature, sentiment in features.items():\n",
    "            if feature not in feature_sentiments:\n",
    "                feature_sentiments[feature] = []\n",
    "            feature_sentiments[feature].append(sentiment['scores']['compound'])\n",
    "    \n",
    "    # Generate feature insights\n",
    "    for feature, sentiments in feature_sentiments.items():\n",
    "        avg_sent = np.mean(sentiments)\n",
    "        insights.append(f\"{feature.capitalize()}: {avg_sent:.2f}\")\n",
    "    \n",
    "    # Generate recommendations\n",
    "    insights.append(\"\\nRecommendations:\")\n",
    "    for feature, sentiments in feature_sentiments.items():\n",
    "        avg_sent = np.mean(sentiments)\n",
    "        if avg_sent < 0:\n",
    "            insights.append(f\"- Improve {feature} quality\")\n",
    "        elif avg_sent > 0.5:\n",
    "            insights.append(f\"- Highlight {feature} in marketing\")\n",
    "    \n",
    "    return '\\n'.join(insights)\n",
    "```\n",
    "\n",
    "### Exercise 7: Complete Analysis System\n",
    "```python\n",
    "def analyze_customer_feedback(category='Electronics', max_reviews=1000):\n",
    "    # Load data\n",
    "    df = load_amazon_reviews(category, max_reviews)\n",
    "    \n",
    "    # Preprocess\n",
    "    df['processed_text'] = df['review_text'].apply(advanced_preprocess)\n",
    "    \n",
    "    # Analyze sentiment\n",
    "    sentiment_scores = df['review_text'].apply(analyze_sentiment_with_confidence)\n",
    "    df['sentiment'] = sentiment_scores.apply(lambda x: x['sentiment'])\n",
    "    df['confidence'] = sentiment_scores.apply(lambda x: x['confidence'])\n",
    "    \n",
    "    # Extract features\n",
    "    df['features'] = df['processed_text'].apply(extract_features_with_sentiment)\n",
    "    \n",
    "    # Generate visualizations\n",
    "    plot_sentiment_distribution(df['confidence'])\n",
    "    \n",
    "    # Generate insights\n",
    "    insights = generate_business_insights({'reviews': df.to_dict('records')})\n",
    "    \n",
    "    return {\n",
    "        'dataframe': df,\n",
    "        'insights': insights\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
